# -*- coding: utf-8 -*-
"""ollama.LLM.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19GvOSfx-eTzbrloJChgoD_dz2x3PYUxN
"""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
import streamlit as st

st.title("Ai chatbot")
input_txt = st.text_input("Please enter your queries here...")

prompt = ChatPromptTemplate.from_messages(
    [("system","you are a helpful AI assistant.Your name is AI Assistant"),
     ("user","user query:{query}")
    ])

llm = Ollama(model="llama2")
output_parser=StrOutputParser()
chain=prompt|llm|output_parser

if input_txt:
    st.write(chain.invoke({"query":input_txt}))

import requests
from requests.exceptions import ConnectionError

try:
    response = requests.get("http://localhost:11434/api/generate")
    print(response.status_code)
except ConnectionError as e:
    print(f"Error: {e}")

!pip install streamlit





!pip install langchain-community

!git --version

!git clone https://github.com/Vswetha05/ollama-chatbot.git

# Commented out IPython magic to ensure Python compatibility.
# %cd ollama-chatbot

!ls

!ls /content

!ls /content

!ls /content/ollama-chatbot